{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework assignment 1\n",
    "Name: Klaas Schuijtemaker\n",
    "\n",
    "Student nr.: 11163119\n",
    "\n",
    "Course: Information Retrieval\n",
    "\n",
    "Date: 11 jan. 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Class definitions\n",
    "We start of by defining some classes and their functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The first class we need is something to represent a document or webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Document/ Webpage \n",
    "class Doc:\n",
    "    # Input:\n",
    "    #   rel: relevance of the document \n",
    "    def __init__(self, rel):\n",
    "        self.rel = rel\n",
    "        \n",
    "    # representation\n",
    "    def __repr__(self):\n",
    "        return 'Doc(' + str(self.rel) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc(3)\n",
      "Relevance: 3\n"
     ]
    }
   ],
   "source": [
    "# Test the Doc -class\n",
    "d = Doc(3)\n",
    "print(d)\n",
    "print('Relevance:', d.rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we need a class to hold the documents returned by a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Query session: a list of documents\n",
    "class QuerySession:\n",
    "    doc_list = []\n",
    "    click_list = []\n",
    "    \n",
    "    # Input:\n",
    "    #   doc_list: list with Docs ordered by rank\n",
    "    def __init__(self, doc_list):\n",
    "        self.doc_list = doc_list\n",
    "        \n",
    "    # representation\n",
    "    def __repr__(self):\n",
    "        return 'QuerySession(' + str(self.doc_list) + ')'\n",
    "    \n",
    "    # Discounted Cumulative Gain\n",
    "    #   Input:\n",
    "    #     k: rank position\n",
    "    #   Output: DCG at rank k\n",
    "    def dcg_evaluation(self, k = 5):\n",
    "        out = 0\n",
    "        for i in range(1, k + 1):\n",
    "            out += (math.pow(2, self.doc_list[i - 1].rel) - 1) / math.log(i + 1, 2)\n",
    "        return out\n",
    "\n",
    "    # Rank Biased Precision\n",
    "    #   Input: \n",
    "    #     p: persistence parameter\n",
    "    #   Output: RBP\n",
    "    def rbp_evaluation(self, p = 0.8):\n",
    "        out = 0\n",
    "        for i in range(1, len(self.doc_list) + 1):\n",
    "            out += self.doc_list[i - 1].rel * math.pow(p, i)\n",
    "        out *= 1 - p\n",
    "        return out\n",
    "    \n",
    "    # Expected Reciprocal Rank\n",
    "    #   Input: \n",
    "    #     gmax: maximum relevance\n",
    "    #   Output: ERR\n",
    "    def err_evaluation(self, gmax = 4):\n",
    "        out = 0\n",
    "        p = 1\n",
    "        for r in range(1, len(self.doc_list) + 1):\n",
    "            ri = (math.pow(2, self.doc_list[r - 1].rel - 1)) / (math.pow(2, gmax))\n",
    "            out += p * ri / r\n",
    "            p *= 1 - ri\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuerySession([Doc(3), Doc(1), Doc(4), Doc(2), Doc(5)])\n",
      "First doc: Doc(3) \n",
      "\n",
      "DCG evaluation: 28.415396452062424\n",
      "RBP evaluation: 1.50912\n",
      "ERR evaluation: 0.463134765625\n"
     ]
    }
   ],
   "source": [
    "# Test the QuerySession -class\n",
    "qs = QuerySession([Doc(3),Doc(1),Doc(4),Doc(2),Doc(5)])\n",
    "print(qs)\n",
    "print('First doc:', qs.doc_list[0], '\\n')\n",
    "\n",
    "# Test evaluations:\n",
    "print('DCG evaluation:', qs.dcg_evaluation())\n",
    "print('RBP evaluation:', qs.rbp_evaluation())\n",
    "print('ERR evaluation:', qs.err_evaluation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next a class to handle interleaving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Pair: two QuerySessions.\n",
    "class Pair:\n",
    "    \n",
    "    # Input:\n",
    "    #   qs1: QuerySession 1\n",
    "    #   qs2: QuerySession 2\n",
    "    def __init__(self, qs1, qs2):\n",
    "        self.qs1 = qs1\n",
    "        self.qs2 = qs2\n",
    "        \n",
    "    # representation\n",
    "    def __repr__(self):\n",
    "        return 'Pair(' + str(self.qs1) + ', ' + str(self.qs2) + ')'\n",
    "    \n",
    "    # Team-Draft Interleaving\n",
    "    #   Input:\n",
    "    #     length: number of documents\n",
    "    #   Output: [i_list, team_a, team_b]\n",
    "    def team_draft_interleaving(self):\n",
    "        i1 = 0\n",
    "        i2 = 0\n",
    "        team_a = []\n",
    "        team_b = []\n",
    "        i_list = []\n",
    "        while i1 < len(self.qs1.doc_list) or i2 < len(self.qs2.doc_list):\n",
    "            if len(team_a) < len(team_b) or (len(team_a) == len(team_b) and random.getrandbits(1) == 1):\n",
    "                if self.qs1.doc_list[i1] not in i_list:\n",
    "                    i_list.append(self.qs1.doc_list[i1])\n",
    "                    team_a.append(self.qs1.doc_list[i1])\n",
    "                i1 += 1\n",
    "            else:\n",
    "                if self.qs2.doc_list[i2] not in i_list:\n",
    "                    i_list.append(self.qs2.doc_list[i2])\n",
    "                    team_b.append(self.qs2.doc_list[i2])\n",
    "                i2 += 1\n",
    "        return {'interleaved': QuerySession(i_list), 'team_a': team_a, 'team_b': team_b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair(QuerySession([Doc(1), Doc(2), Doc(3), Doc(4)]), QuerySession([Doc(4), Doc(3), Doc(2), Doc(1)])) \n",
      "\n",
      "Team-Draft Interleaving: QuerySession([Doc(4), Doc(1), Doc(3), Doc(2), Doc(2), Doc(3), Doc(1), Doc(4)])\n",
      "Team A: [Doc(1), Doc(2), Doc(3), Doc(4)]\n",
      "Team B: [Doc(4), Doc(3), Doc(2), Doc(1)]\n"
     ]
    }
   ],
   "source": [
    "# Test the Pair -class\n",
    "qs1 = QuerySession([Doc(1),Doc(2),Doc(3),Doc(4)])\n",
    "qs2 = QuerySession([Doc(4),Doc(3),Doc(2),Doc(1)])\n",
    "pair = Pair(qs1, qs2)\n",
    "print(pair, '\\n')\n",
    "\n",
    "# Test Team-Draft Interleaving:\n",
    "inter = pair.team_draft_interleaving()\n",
    "print('Team-Draft Interleaving:', inter['interleaved'])\n",
    "print('Team A:', inter['team_a'])\n",
    "print('Team B:', inter['team_b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we need a class handle many pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pair-list: unordered list containing Pairs: [Pair5, Pair3, Pair2, ...]\n",
    "class PairList(list):\n",
    " \n",
    "    # representation\n",
    "    def __repr__(self):\n",
    "        return 'PairList([\\n' + ',\\n'.join(str(d) for d in self[0:2]) + ',\\n...,\\n' + ',\\n'.join(str(d) for d in self[-3:-1]) + '\\n])'\n",
    "\n",
    "    # Split PairList into multiple PairList\n",
    "    #   Input:\n",
    "    #     no_groups: split into num of groups\n",
    "    #     function: evaluation measure that is used to split\n",
    "    #   Output: a list containing PairLists: [PairList3, PairList2, PairList1, ...]\n",
    "    def split(self, no_groups, function):\n",
    "        measure = []\n",
    "        for i in range(len(self)):\n",
    "            pair = self[i]\n",
    "            measure.append(function(pair.qs2) - function(pair.qs1)) # ð›¥measure = measure_e - measure_p\n",
    "\n",
    "        group_range = max(measure) / no_groups # 0 < ð›¥measure_group1 â‰¤ group_range * 1 â‰¤ ð›¥measure_group2 â‰¤ group_range * 2 ...\n",
    "        group = [PairList([]) for i in range(no_groups)] # group = [[],[],[], ...]\n",
    "        for i in range(len(self)):\n",
    "            if measure[i] > 0:\n",
    "                group_index = int(math.ceil(measure[i] / group_range) - 1) # group_index range from 0 to 10\n",
    "                group[group_index].append(self[i])\n",
    "        return group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 1: Simulate Rankings of Relevance for E and P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment said that the relavance point scale should range from 0 to 5. However, this range resulted in a PairList that was simely too big to handle. Therefore the point scale range was lowered and now ranges from 0 to 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_scale = 3      # relevance point scale, ranges from 0 to 3\n",
    "\n",
    "p = e = []\n",
    "for i1 in range(point_scale):\n",
    "    for i2 in range(point_scale):\n",
    "        for i3 in range(point_scale):\n",
    "            for i4 in range(point_scale):\n",
    "                for i5 in range(point_scale):\n",
    "                    p.append(QuerySession([Doc(i1), Doc(i2), Doc(i3), Doc(i4), Doc(i5)]))\n",
    "\n",
    "pairs = PairList([])\n",
    "for i1 in range(len(p)):\n",
    "    for i2 in range(len(e)):\n",
    "        pairs.append(Pair(p[i1], e[i2])) # add all possible combinations for P with E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PairList([\n",
      "Pair(QuerySession([Doc(0), Doc(0), Doc(0), Doc(0), Doc(0)]), QuerySession([Doc(0), Doc(0), Doc(0), Doc(0), Doc(0)])),\n",
      "Pair(QuerySession([Doc(0), Doc(0), Doc(0), Doc(0), Doc(0)]), QuerySession([Doc(0), Doc(0), Doc(0), Doc(0), Doc(1)])),\n",
      "...,\n",
      "Pair(QuerySession([Doc(2), Doc(2), Doc(2), Doc(2), Doc(2)]), QuerySession([Doc(2), Doc(2), Doc(2), Doc(2), Doc(0)])),\n",
      "Pair(QuerySession([Doc(2), Doc(2), Doc(2), Doc(2), Doc(2)]), QuerySession([Doc(2), Doc(2), Doc(2), Doc(2), Doc(1)]))\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "# Check some of the pairs\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Divide pairs into groups of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_groups = 10 # number of groups\n",
    "\n",
    "dcg_groups = pairs.split(no_groups, QuerySession.dcg_evaluation)\n",
    "rbp_groups = pairs.split(no_groups, QuerySession.rbp_evaluation)\n",
    "err_groups = pairs.split(no_groups, QuerySession.err_evaluation)\n",
    "\n",
    "all_groups = [dcg_groups, rbp_groups, err_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcg_group[0]:\n",
      "PairList([\n",
      "Pair(QuerySession([Doc(0), Doc(0), Doc(0), Doc(0), Doc(0)]), QuerySession([Doc(0), Doc(0), Doc(0), Doc(0), Doc(1)])),\n",
      "Pair(QuerySession([Doc(0), Doc(0), Doc(0), Doc(0), Doc(0)]), QuerySession([Doc(0), Doc(0), Doc(0), Doc(1), Doc(0)])),\n",
      "...,\n",
      "Pair(QuerySession([Doc(2), Doc(2), Doc(2), Doc(2), Doc(0)]), QuerySession([Doc(2), Doc(2), Doc(2), Doc(1), Doc(2)])),\n",
      "Pair(QuerySession([Doc(2), Doc(2), Doc(2), Doc(2), Doc(0)]), QuerySession([Doc(2), Doc(2), Doc(2), Doc(2), Doc(1)]))\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "# Check the Discounted Cumulative Gain -group\n",
    "print('dcg_group[0]:')\n",
    "print(dcg_groups[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Implement Team-Draft Interleaving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interleaving is already included in the class Pair. We now replace all pairs in all groups with the interleaved ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace pairs by interleaves\n",
    "for groups in all_groups:\n",
    "    for i, group in enumerate(groups):\n",
    "        i_list = []\n",
    "        for pair in group:\n",
    "            i_list.append(pair.team_draft_interleaving())\n",
    "        groups[i] = i_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG num. of groups: 10\n",
      "DCG size of group[0]: 7839\n",
      "DCG size of group[0][0]: 3\n"
     ]
    }
   ],
   "source": [
    "# There should be 10 Discounted Cumulative Gain -groups:\n",
    "print('DCG num. of groups:', len(dcg_groups))\n",
    "\n",
    "# Each group has a variable amount of interleaves:\n",
    "print('DCG size of group[0]:', len(dcg_groups[0]))\n",
    "\n",
    "# One interleaf has an interleaved-ranking, a team A and a team B:\n",
    "print('DCG size of group[0][0]:', len(dcg_groups[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Simulate User Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add support for nested dictonaries: {key1: {key2: value2}}\n",
    "class NestedDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        value = self[key] = type(self)()\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First we define a function to read a Yandex Click Log File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load a Yandex Click Log File:\n",
    "#   Input:\n",
    "#     file: Yandex Click Log File\n",
    "#   Output: session_map[session_id][query_id] containing alls QuerySession in the Yandex file\n",
    "def load_yandex_click_log(file):\n",
    "    session_map = NestedDict()\n",
    "    session_id = 0\n",
    "    for line in file:\n",
    "        cells = line.split('\\t')\n",
    "        \n",
    "        new_session_id = int(cells[0])\n",
    "        if session_id != new_session_id:\n",
    "            session_id = new_session_id\n",
    "            \n",
    "        if cells[2] == 'Q':\n",
    "            query_id = int(cells[3])\n",
    "            if query_id in session_map[session_id]:\n",
    "                session = session_map[session_id][query_id]\n",
    "            else:\n",
    "                session = QuerySession([int(c.strip()) for c in cells[5:]])\n",
    "                session.click_list = [0] * len(session.doc_list)\n",
    "                session_map[session_id][query_id] = session\n",
    "        \n",
    "        if cells[2] == 'C':\n",
    "            doc_id = int(cells[3].strip())\n",
    "            for s in session_map[session_id].values():\n",
    "                if doc_id in s.doc_list: \n",
    "                    rank = s.doc_list.index(doc_id)\n",
    "                    s.click_list[rank] += 1\n",
    "                    break\n",
    "    return session_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Implement the click models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random click model estimate the probability of a click\n",
    "#   Input:\n",
    "#     session_map[session_id][query_id]\n",
    "#   Output: Rho, the probability of a click\n",
    "def rcm_get_click_prob(session_map):\n",
    "    click_count = 0\n",
    "    doc_count = 0\n",
    "    for query_map in session_map.values():\n",
    "        for session_query in query_map.values():\n",
    "            for clicks in session_query.click_list:\n",
    "                click_count += clicks                  # Count the number of clicks\n",
    "        doc_count += len(session_query.doc_list)       # Count the number of documents\n",
    "    return click_count / doc_count                     # Rho = click_count / doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simplified Dependent Click Model estimate the continuation probability\n",
    "#   Input:\n",
    "#     session_map[session_id][query_id]\n",
    "#   Output: Lambda r, a list[rank] with the continuation probability at each rank\n",
    "def sdcm_get_continuation_probs(session_map):\n",
    "    click_list = [0] * 10\n",
    "    click_not_last_list = [0] * 10\n",
    "    for query_map in session_map.values():\n",
    "        for session_query in query_map.values():\n",
    "            click_ranks = [r for r, click in enumerate(session_query.click_list) if click]\n",
    "            if len(click_ranks) > 0:\n",
    "                click_list[click_ranks[-1]] += session_query.click_list[click_ranks[-1]]\n",
    "                for rank in click_ranks[:-1]:\n",
    "                    click_list[rank] += session_query.click_list[rank]\n",
    "                    click_not_last_list[rank] += session_query.click_list[rank]\n",
    "    return [a / b for a, b in zip(click_not_last_list, click_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplified Dependent Click model estimate the attraction probability\n",
    "#   Input:\n",
    "#     query_session\n",
    "#   Output: Alpha r, a list[rank] with the attraction probability at each rank\n",
    "def sdcm_get_attraction_probs(query_session):\n",
    "    return [d.rel / 4 for d in query_session.doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide -stochastically- whether a document was clicked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random click model, estimate whether a document is clicked\n",
    "#   Input:\n",
    "#     rcm_click_prob: the probability of a click\n",
    "#   Output: True if this document was clicked, False otherwise\n",
    "def bernoulli(click_prob):\n",
    "    return random.random() < click_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simplified Dependent Click model, estimate whether a document is clicked\n",
    "#   Input:\n",
    "#     query_session: a query session containing a ranked list of documents\n",
    "#     cont_probs: Lambda r, a list[rank] with the continuation probability at each rank\n",
    "#     attr_probs: Alpha r, a list[rank] with the attraction probability at each rank\n",
    "#   Output: A click list[rank]: [True, False, False]\n",
    "def sdcm_was_clicked(query_session, cont_probs, attr_probs):\n",
    "    exam = 1\n",
    "    click_list = []\n",
    "    for rank in range(len(query_session.doc_list)):\n",
    "        cont = cont_probs[rank]\n",
    "        attr = attr_probs[rank]\n",
    "        if exam == 0:\n",
    "            click = False\n",
    "            exam = 0\n",
    "        else:\n",
    "            click = bernoulli(attr)\n",
    "            exam = bernoulli(1 - click + cont * click)\n",
    "        click_list.append(click)\n",
    "    return click_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all functions are defined, it is time to load the Yandex-file and calulate some probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load file\n",
    "f = open('YandexRelPredChallenge.txt', 'r')\n",
    "session_map = load_yandex_click_log(f)\n",
    "f.close()\n",
    "\n",
    "# calculate probabilities\n",
    "rcm_click_prob = rcm_get_click_prob(session_map)\n",
    "sdcm_cont_probs = sdcm_get_continuation_probs(session_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random click probability: 0.49\n",
      "Continuation probabilities: [0.39, 0.57, 0.6, 0.59, 0.58, 0.57, 0.54, 0.46, 0.38, 0.0] \n",
      "\n",
      "Test QuerySession([Doc(3), Doc(1), Doc(4), Doc(2), Doc(1)])\n",
      "Test Attraction probabilities: [0.75, 0.25, 1.0, 0.5, 0.25]\n",
      "Test clicked-list: [True, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Check probabilities\n",
    "print('Random click probability:', round(rcm_click_prob, 2))\n",
    "print('Continuation probabilities:', [round(c, 2) for c in sdcm_cont_probs], '\\n')\n",
    "\n",
    "# Test Simplified Dependent Click Model\n",
    "qs = QuerySession([Doc(3),Doc(1),Doc(4),Doc(2),Doc(1)])\n",
    "sdcm_attr_probs = sdcm_get_attraction_probs(qs)\n",
    "sdcm_click_list = sdcm_was_clicked(qs, sdcm_cont_probs, sdcm_attr_probs)\n",
    "print('Test', qs)\n",
    "print('Test Attraction probabilities:', sdcm_attr_probs)\n",
    "print('Test clicked-list:', sdcm_click_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Simulate Interleaving Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_simulations = 50 # the number of simulations\n",
    "\n",
    "try:\n",
    "    for groups in all_groups:\n",
    "        for group in groups:\n",
    "            for inter in group:\n",
    "                interleaved = inter['interleaved']\n",
    "                team_a = inter['team_a']\n",
    "                team_b = inter['team_b']\n",
    "\n",
    "                # Random Click Model  x 50\n",
    "                p_win_count = 0\n",
    "                e_win_count = 0\n",
    "                for i in range(no_simulations):\n",
    "                    for doc in interleaved.doc_list:\n",
    "                        if bernoulli(rcm_click_prob):\n",
    "                            if doc in team_a:\n",
    "                                p_win_count += 1\n",
    "                            else:\n",
    "                                if doc in team_b:\n",
    "                                    e_win_count += 1\n",
    "                                else:\n",
    "                                    print('Error1: doc not part of A or B')\n",
    "                                    raise LookupError\n",
    "                \n",
    "                # Save the proportion that E has won\n",
    "                inter['rcm_e_prop'] = e_win_count / (p_win_count + e_win_count)\n",
    "                \n",
    "                # Simplified Dependent Click Model  x 50\n",
    "                p_win_count = 0\n",
    "                e_win_count = 0\n",
    "                for i in range(no_simulations):\n",
    "                    sdcm_attr_probs = sdcm_get_attraction_probs(interleaved)\n",
    "                    click_list = sdcm_was_clicked(interleaved, sdcm_cont_probs, sdcm_attr_probs)\n",
    "                    for rank, click in enumerate(click_list):\n",
    "                        if click:\n",
    "                            doc = interleaved.doc_list[rank]\n",
    "                            if doc in team_a:\n",
    "                                p_win_count += 1\n",
    "                            else:\n",
    "                                if doc in team_b:\n",
    "                                    e_win_count += 1\n",
    "                                else:\n",
    "                                    print('Error2: doc not part of A or B')\n",
    "                                    raise LookupError\n",
    "                \n",
    "                # Save the proportion that E has won\n",
    "                inter['sdcm_e_prop'] = e_win_count / (p_win_count + e_win_count)\n",
    "                \n",
    "except LookupError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of E wins in the very first RCM-test: 0.524\n",
      "Proportion of E wins in the very first SDCM-test: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check the proportion of E wins in the first interleaved ranking from the Discounted Cumulative Gain -group\n",
    "\n",
    "# In the Random Click Model\n",
    "print('Proportion of E wins in the very first RCM-test:', dcg_groups[0][0]['rcm_e_prop'])\n",
    "\n",
    "# In the Simplified Dependent Click Model\n",
    "print('Proportion of E wins in the very first SDCM-test:', dcg_groups[0][0]['sdcm_e_prop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 6: Compute Sample Size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_alpha = 1.645 # 5%\n",
    "z_beta = 1.282 # 10%\n",
    "p0 = 0.5\n",
    "\n",
    "for groups in all_groups:\n",
    "    for group in groups:\n",
    "        for inter in group:\n",
    "            # Compute Random Click Model sample size\n",
    "            p = inter['rcm_e_prop']\n",
    "            delta = p - p0\n",
    "            if delta == 0:\n",
    "                inter['rcm_sample_size'] = -1\n",
    "            else:\n",
    "                inter['rcm_sample_size'] = math.pow((z_alpha * math.sqrt(p0 * (1 - p0)) + z_beta * math.sqrt(p * (1 - p))) / abs(delta), 2)\n",
    "                #inter['rcm_sample_size'] += 1 / delta # continuity correction\n",
    "                \n",
    "            # Compute Simplified Dependent Click Model sample size\n",
    "            p = inter['sdcm_e_prop']\n",
    "            delta = p - p0\n",
    "            if delta == 0:\n",
    "                inter['sdcm_sample_size'] = -1\n",
    "            else:\n",
    "                inter['sdcm_sample_size'] = math.pow((z_alpha * math.sqrt(p0 * (1 - p0)) + z_beta * math.sqrt(p * (1 - p))) / abs(delta), 2)\n",
    "                #inter['sdcm_sample_size'] += 1 / delta # continuity correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_repr_quantile(array, q):\n",
    "    return str(round(array[round((len(array) - 1) * q)], 2))\n",
    "\n",
    "def print_quantiles(name, array):\n",
    "    array.sort()\n",
    "    print(name, 'sample size quantiles:')\n",
    "    print('min:\\t',    get_repr_quantile(array, 0.00))\n",
    "    print('q5:\\t',     get_repr_quantile(array, 0.05))\n",
    "    print('median:\\t', get_repr_quantile(array, 0.50))\n",
    "    print('q95:\\t',    get_repr_quantile(array, 0.95))\n",
    "    print('max:\\t',    get_repr_quantile(array, 1.00))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discounted Cumulative Gain sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 24.11\n",
      "median:\t 1822.46\n",
      "q95:\t 131727.5\n",
      "max:\t 666885.7\n",
      "\n",
      "Rank Biased Precision sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 24.55\n",
      "median:\t 1828.72\n",
      "q95:\t 131727.5\n",
      "max:\t 745568.05\n",
      "\n",
      "Expected Reciprocal Rank sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 24.2\n",
      "median:\t 1851.16\n",
      "q95:\t 131727.5\n",
      "max:\t 676481.11\n",
      "\n",
      "Random Click Model sample size quantiles:\n",
      "min:\t 165.95\n",
      "q5:\t 1032.89\n",
      "median:\t 8229.45\n",
      "q95:\t 489370.65\n",
      "max:\t 745568.05\n",
      "\n",
      "Simplified Dependent Click Model sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 15.71\n",
      "median:\t 162.97\n",
      "q95:\t 11724.92\n",
      "max:\t 101784.68\n",
      "\n",
      "Group 1 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 125.78\n",
      "median:\t 2905.82\n",
      "q95:\t 133860.76\n",
      "max:\t 686145.06\n",
      "\n",
      "Group 2 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 60.58\n",
      "median:\t 2087.88\n",
      "q95:\t 131727.5\n",
      "max:\t 745568.05\n",
      "\n",
      "Group 3 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 34.02\n",
      "median:\t 1467.67\n",
      "q95:\t 131727.5\n",
      "max:\t 666885.7\n",
      "\n",
      "Group 4 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 21.59\n",
      "median:\t 1032.89\n",
      "q95:\t 131727.5\n",
      "max:\t 676481.11\n",
      "\n",
      "Group 5 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 14.97\n",
      "median:\t 627.85\n",
      "q95:\t 127512.37\n",
      "max:\t 647900.5\n",
      "\n",
      "Group 6 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 11.36\n",
      "median:\t 290.14\n",
      "q95:\t 129611.37\n",
      "max:\t 676481.11\n",
      "\n",
      "Group 7 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 8.48\n",
      "median:\t 104.78\n",
      "q95:\t 133860.76\n",
      "max:\t 647900.5\n",
      "\n",
      "Group 8 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 6.14\n",
      "median:\t 54.08\n",
      "q95:\t 136011.16\n",
      "max:\t 601636.93\n",
      "\n",
      "Group 9 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 2.71\n",
      "median:\t 28.88\n",
      "q95:\t 119287.74\n",
      "max:\t 647900.5\n",
      "\n",
      "Group 10 sample size quantiles:\n",
      "min:\t 2.71\n",
      "q5:\t 2.71\n",
      "median:\t 10.04\n",
      "q95:\t 59968.5\n",
      "max:\t 514250.17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# report measure sample sizes\n",
    "measure_names = ['Discounted Cumulative Gain', 'Rank Biased Precision', 'Expected Reciprocal Rank']\n",
    "for i, groups in enumerate(all_groups):\n",
    "    sample_sizes = []\n",
    "    for group in groups:\n",
    "        for inter in group:\n",
    "            if inter['rcm_sample_size'] != -1:\n",
    "                sample_sizes.append(inter['rcm_sample_size'])\n",
    "            if inter['sdcm_sample_size'] != -1:\n",
    "                sample_sizes.append(inter['sdcm_sample_size'])\n",
    "    print_quantiles(measure_names[i], sample_sizes)\n",
    "    \n",
    "# report click model sample sizes\n",
    "rcm_sample_sizes = []\n",
    "sdcm_sample_sizes = []\n",
    "for groups in all_groups:\n",
    "    for group in groups:\n",
    "        for inter in group:\n",
    "            if inter['rcm_sample_size'] != -1:\n",
    "                rcm_sample_sizes.append(inter['rcm_sample_size'])\n",
    "            if inter['sdcm_sample_size'] != -1:\n",
    "                sdcm_sample_sizes.append(inter['sdcm_sample_size'])\n",
    "print_quantiles('Random Click Model', rcm_sample_sizes)\n",
    "print_quantiles('Simplified Dependent Click Model', sdcm_sample_sizes)\n",
    "\n",
    "# report group sample sizes\n",
    "sample_sizes_array = [[] for i in range(10)]\n",
    "for groups in all_groups:\n",
    "    for i, group in enumerate(groups):\n",
    "        sample_sizes = sample_sizes_array[i]\n",
    "        for inter in group:\n",
    "            if inter['rcm_sample_size'] != -1:\n",
    "                sample_sizes.append(inter['rcm_sample_size'])\n",
    "            if inter['sdcm_sample_size'] != -1:\n",
    "                sample_sizes.append(inter['sdcm_sample_size'])\n",
    "for i, sample_sizes in enumerate(sample_sizes_array):\n",
    "    print_quantiles('Group ' + str(i + 1), sample_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "The entire test simulates two non-existing algorithms named E and P. E is an experimental algorithm that should replace P. Before P can be replaced, E must proof to be better. This is done tests. But how many tests are needed to proof E better? This entire simulation is about determine the sample size needed to proof that E is better than P.\n",
    "\n",
    "In each test, algorithm E was deliberately given an advantage over algorithm P. The tests were divided over 10 groups. E had a small advantage in group 1 and E had a large advantage in group 10. After all tests the sample size, needed to proof that E better, was calculated. It turned out that in group 1, a â€˜largeâ€™ median sample size of 2905 is needed. While in group 10, a â€˜smallâ€™ median sample size of 10 is needed. This is as expected; when there isnâ€™t much difference between algorithms, the amount of tests needed to proof one better than the other, must be â€˜largeâ€™.\n",
    "\n",
    "Two click models were tested, the random click model (RCM) and the simplified dependent click model (SDCM). Because the RCM is almost entirely random, it canâ€™t create a big difference between E and P. The median sample size found for the RCM is 8229 which is â€˜largeâ€™. The SDCM seems to create a big difference between E and P and has a median sample size of 163 which is â€˜smallâ€™.\n",
    "\n",
    "Three evaluation methods were tested, the Discounted Cumulative Gain (DCG), the Rank Biased Precision (RBP) and the Expected Reciprocal Rank (ERR). All three evaluation methods scored similar with a median sample size of +-1830 which is â€˜averageâ€™. A good evaluation method is preferred to create big difference between algorithms so that the amount of tests needed is minimal.\n",
    "\n",
    "### Drawback\n",
    "This method is good for giving an approximate of the amount of tests needed. However, the difference between the minimum and the maximum sample size is always big. For example, in group 10, the median sample size of 10 is 'small'. But the maximum sample size is 514250 which is 'incredibly large'. This raises the question: are 10 tests really going to be sufficient?\n",
    "\n",
    "### Alternative\n",
    "Instead of determining the sample size, one could decide to start the test immediately and let it run until a statistical difference between the two algorithms have been found.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
