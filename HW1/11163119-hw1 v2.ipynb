{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW1\n",
    "Name: Klaas Schuijtemaker\n",
    "\n",
    "Student nr.: 11163119\n",
    "\n",
    "Course: Information Retrieval\n",
    "\n",
    "Date: 18 jan. 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Simulate Rankings of Relevance for E and P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define several classes that will be used throughout the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Document \n",
    "class Doc:\n",
    "    id = 0     # document id\n",
    "    rel = 'N'  # document relevance {N,R,HR}\n",
    "    \n",
    "    def __init__(self, id, rel):\n",
    "        self.id = id\n",
    "        self.rel = rel\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Doc(' + str(self.id) + ',' + str(self.rel) + ')'\n",
    "    \n",
    "    def __eq__(self, doc2):\n",
    "        return self.id == doc2.id\n",
    "    \n",
    "    def relevance(self):\n",
    "        if self.rel == 'N':\n",
    "            return 0\n",
    "        elif self.rel == 'R':\n",
    "            return 1\n",
    "        elif self.rel == 'HR':\n",
    "            return 5\n",
    "        else:\n",
    "            raise LookupError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc(0,HR)\n",
      "Document relevance: 5\n"
     ]
    }
   ],
   "source": [
    "# Test Doc -class\n",
    "d = Doc(0,'HR')\n",
    "print(d)\n",
    "print('Document relevance:', d.relevance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query session: a list of documents\n",
    "class QuerySession:\n",
    "    doc_list = []   # list with Docs ordered by rank\n",
    "    click_list = [] # list containigh the number of clicks per document\n",
    "    \n",
    "    def __init__(self, doc_list):\n",
    "        self.doc_list = doc_list\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'QuerySession(' + str(self.doc_list) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuerySession([Doc(0,R), Doc(1,R), Doc(2,R), Doc(3,N), Doc(4,R)])\n",
      "First doc: Doc(0,R)\n"
     ]
    }
   ],
   "source": [
    "# Test QuerySession -class\n",
    "qs = QuerySession([Doc(0,'R'),Doc(1,'R'),Doc(2,'R'),Doc(3,'N'),Doc(4,'R')])\n",
    "print(qs)\n",
    "print('First doc:', qs.doc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pair: two QuerySessions.\n",
    "class Pair:\n",
    "    P = None # Production QuerySession\n",
    "    E = None # Experimental QuerySession\n",
    "    \n",
    "    ap_measure = 0  # Average Precision -ùõ•measure\n",
    "    dcg_measure = 0 # Discounted Cumulative Gain -ùõ•measure\n",
    "    rbp_measure = 0 # Rank Biased Precision -ùõ•measure\n",
    "    err_measure = 0 # Expected Reciprocal Rank -ùõ•measure\n",
    "    \n",
    "    td_interleaving = None # Team-Draft Interleaving\n",
    "    p_interleaving = None  # Probabilistic Interleaving\n",
    "    \n",
    "    click_prop_td_rcm = 0  # The proportion of clicks on E with Team-Draft Interleaving and Random Click Model\n",
    "    click_prop_td_sdcm = 0 # The proportion of clicks on E with Team-Draft Interleaving and Simplified Dependent Click model\n",
    "    click_prop_p_rcm = 0   # The proportion of clicks on E with Probabilistic Interleaving and Random Click Model\n",
    "    click_prop_p_sdcm = 0  # The proportion of clicks on E with Probabilistic Interleaving and Simplified Dependent Click model\n",
    "\n",
    "    def __init__(self, P, E):\n",
    "        self.P = P\n",
    "        self.E = E\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Pair(P:' + str(self.P) + ',\\n     E:' + str(self.E) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair(P:QuerySession([Doc(0,R), Doc(1,N), Doc(2,R), Doc(3,N)]),\n",
      "     E:QuerySession([Doc(4,R), Doc(5,R), Doc(6,R), Doc(7,N)]))\n",
      "Production: QuerySession([Doc(0,R), Doc(1,N), Doc(2,R), Doc(3,N)])\n"
     ]
    }
   ],
   "source": [
    "# Test Pair -class\n",
    "P = QuerySession([Doc(0,'R'),Doc(1,'N'),Doc(2,'R'),Doc(3,'N')])\n",
    "E = QuerySession([Doc(4,'R'),Doc(5,'R'),Doc(6,'R'),Doc(7,'N')])\n",
    "pair = Pair(P, E)\n",
    "print(pair)\n",
    "print('Production:', pair.P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we generate all pairs of rankings of relevance, for both the production P and experimental E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate all QuerySessions\n",
    "graded_relevances = ['N','R','HR']\n",
    "qs_array = []\n",
    "doc_id = 0\n",
    "\n",
    "for i1 in graded_relevances:\n",
    "    for i2 in graded_relevances:\n",
    "        for i3 in graded_relevances:\n",
    "            for i4 in graded_relevances:\n",
    "                for i5 in graded_relevances:\n",
    "                    doc_list = [Doc(doc_id,i1), Doc(doc_id+1,i2), Doc(doc_id+2,i3), Doc(doc_id+3,i4), Doc(doc_id+4,i5)]\n",
    "                    qs_array += [QuerySession(doc_list)]\n",
    "                    doc_id += 5\n",
    "\n",
    "# Add all possible combinations for P with E\n",
    "pairs = []\n",
    "for i1 in range(len(qs_array)):\n",
    "    for i2 in range(len(qs_array)):\n",
    "        pairs += [Pair(qs_array[i1], qs_array[i2])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs: 59049\n",
      "Pair(P:QuerySession([Doc(0,N), Doc(1,N), Doc(2,N), Doc(3,N), Doc(4,N)]),\n",
      "     E:QuerySession([Doc(0,N), Doc(1,N), Doc(2,N), Doc(3,N), Doc(4,N)]))\n",
      "Pair(P:QuerySession([Doc(0,N), Doc(1,N), Doc(2,N), Doc(3,N), Doc(4,N)]),\n",
      "     E:QuerySession([Doc(5,N), Doc(6,N), Doc(7,N), Doc(8,N), Doc(9,R)]))\n"
     ]
    }
   ],
   "source": [
    "# Check some pairs\n",
    "print('Number of pairs:', len(pairs))\n",
    "print(pairs[0])\n",
    "print(pairs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Average Precision\n",
    "#   Input:\n",
    "#     doc_list: [Doc1, Doc2, ...]\n",
    "#   Output: Average Precision\n",
    "def ap_evaluation(doc_list):\n",
    "    total_rel = 0\n",
    "    out = 0\n",
    "    for i in range(len(doc_list)):\n",
    "        total_rel += 0 if doc_list[i].relevance() == 0 else 1\n",
    "        out += total_rel / (i + 1)\n",
    "    return 0 if total_rel == 0 else out / len(doc_list)\n",
    "\n",
    "# Discounted Cumulative Gain\n",
    "#   Input:\n",
    "#     doc_list: [Doc1, Doc2, ...]\n",
    "#     k: rank position\n",
    "#   Output: DCG at rank k\n",
    "def dcg_evaluation(doc_list, k=5):\n",
    "    out = 0\n",
    "    for i in range(1, k + 1):\n",
    "        out += (2**doc_list[i - 1].relevance() - 1) / math.log(i + 1, 2)\n",
    "    return out\n",
    "\n",
    "# Rank Biased Precision\n",
    "#   Input: \n",
    "#     doc_list: [Doc1, Doc2, ...]\n",
    "#     p: persistence parameter\n",
    "#   Output: RBP\n",
    "def rbp_evaluation(doc_list, p=0.8):\n",
    "    out = 0\n",
    "    for i in range(1, len(doc_list) + 1):\n",
    "        out += doc_list[i - 1].relevance() * p**i\n",
    "    out *= 1 - p\n",
    "    return out\n",
    "\n",
    "# Expected Reciprocal Rank\n",
    "#   Input: \n",
    "#     doc_list: [Doc1, Doc2, ...]\n",
    "#     gmax: maximum relevance\n",
    "#   Output: ERR\n",
    "def err_evaluation(doc_list, gmax=4):\n",
    "    out = 0\n",
    "    p = 1\n",
    "    for r in range(1, len(doc_list) + 1):\n",
    "        ri = 2**(doc_list[r - 1].relevance() - 1) / 2**gmax\n",
    "        out += p * ri / r\n",
    "        p *= 1 - ri\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP evaluation: 0.9099999999999999\n",
      "DCG evaluation: 2.517782560805999\n",
      "RBP evaluation: 0.45593599999999995\n",
      "ERR evaluation: 0.12652254104614258\n"
     ]
    }
   ],
   "source": [
    "# Test evaluations:\n",
    "qs = QuerySession([Doc(0,'R'),Doc(1,'R'),Doc(2,'R'),Doc(3,'N'),Doc(4,'R')])\n",
    "print('AP evaluation:', ap_evaluation(qs.doc_list))\n",
    "print('DCG evaluation:', dcg_evaluation(qs.doc_list))\n",
    "print('RBP evaluation:', rbp_evaluation(qs.doc_list))\n",
    "print('ERR evaluation:', err_evaluation(qs.doc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the ùõ•measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate ùõ•measure = measure_e - measure_p\n",
    "for pair in pairs:\n",
    "    pair.ap_measure = ap_evaluation(pair.E.doc_list) - ap_evaluation(pair.P.doc_list)\n",
    "    pair.dcg_measure = dcg_evaluation(pair.E.doc_list) - dcg_evaluation(pair.P.doc_list)\n",
    "    pair.rbp_measure = rbp_evaluation(pair.E.doc_list) - rbp_evaluation(pair.P.doc_list)\n",
    "    pair.err_measure = err_evaluation(pair.E.doc_list) - err_evaluation(pair.P.doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second pair:\n",
      "Pair(P:QuerySession([Doc(0,N), Doc(1,N), Doc(2,N), Doc(3,N), Doc(4,N)]),\n",
      "     E:QuerySession([Doc(5,N), Doc(6,N), Doc(7,N), Doc(8,N), Doc(9,R)]))\n",
      "ùõ•measure = Average_Precision(E) - Average_Precision(P): 0.04\n"
     ]
    }
   ],
   "source": [
    "# Check ùõ•measure\n",
    "print('Second pair:')\n",
    "print(pairs[1])\n",
    "print('ùõ•measure = Average_Precision(E) - Average_Precision(P):', pairs[1].ap_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Team-Draft Interleaving\n",
    "#   Input:  pair\n",
    "#   Output: {'interleaved':i_list, 'team_a':team_a, 'team_b':team_b}\n",
    "def td_interleaving(pair):\n",
    "    i1 = 0\n",
    "    i2 = 0\n",
    "    team_a = []\n",
    "    team_b = []\n",
    "    i_list = []\n",
    "    while i1 < len(pair.P.doc_list) or i2 < len(pair.E.doc_list):\n",
    "        if i1 < i2 or (i1 == i2 and random.getrandbits(1) == 1):\n",
    "            i_list += [pair.P.doc_list[i1]]\n",
    "            team_a += [pair.P.doc_list[i1]]\n",
    "            i1 += 1\n",
    "        else:\n",
    "            i_list += [pair.E.doc_list[i2]]\n",
    "            team_b += [pair.E.doc_list[i2]]\n",
    "            i2 += 1\n",
    "\n",
    "        while i1 < len(pair.P.doc_list) and pair.P.doc_list[i1] in i_list:\n",
    "            i1 += 1\n",
    "        while i2 < len(pair.E.doc_list) and pair.E.doc_list[i2] in i_list:\n",
    "            i2 += 1\n",
    "    return {'interleaved': QuerySession(i_list), 'team_a': team_a, 'team_b': team_b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Probabilistic Interleaving\n",
    "#   Input:  pair\n",
    "#   Output: {'interleaved':i_list, 'team_a':team_a, 'team_b':team_b}\n",
    "def p_interleaving(pair, t=3):\n",
    "    # TODO: ...\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with Team-Draft Interleaving:\n",
      "Interleaved: QuerySession([Doc(1,R), Doc(5,R), Doc(3,N), Doc(2,R), Doc(6,R), Doc(4,R), Doc(7,R)])\n",
      "Team A: [Doc(1,R), Doc(2,R), Doc(4,R)]\n",
      "Team B: [Doc(5,R), Doc(3,N), Doc(6,R), Doc(7,R)]\n"
     ]
    }
   ],
   "source": [
    "# Test Team-Draft Interleaving:\n",
    "P = QuerySession([Doc(1,'R'),Doc(2,'R'),Doc(3,'N'),Doc(4,'R')])\n",
    "E = QuerySession([Doc(5,'R'),Doc(3,'N'),Doc(6,'R'),Doc(7,'R')])\n",
    "pair = Pair(P, E)\n",
    "\n",
    "td = td_interleaving(pair)\n",
    "print('Test with Team-Draft Interleaving:')\n",
    "print('Interleaved:', td['interleaved'])\n",
    "print('Team A:', td['team_a'])\n",
    "print('Team B:', td['team_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do interleaving on all pairs\n",
    "for pair in pairs:\n",
    "    pair.td_interleaving = td_interleaving(pair)\n",
    "    pair.p_interleaving = p_interleaving(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interleaved': QuerySession([Doc(250,N), Doc(0,N), Doc(251,R), Doc(1,N), Doc(252,HR), Doc(2,N), Doc(3,N), Doc(253,R), Doc(4,N), Doc(254,HR)]),\n",
       " 'team_a': [Doc(0,N), Doc(1,N), Doc(2,N), Doc(3,N), Doc(4,N)],\n",
       " 'team_b': [Doc(250,N), Doc(251,R), Doc(252,HR), Doc(253,R), Doc(254,HR)]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Team-Draft interleaving on a pair\n",
    "pairs[50].td_interleaving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement User Clicks Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add support for nested dictonaries: {key1: {key2: value2}}\n",
    "class NestedDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        value = self[key] = type(self)()\n",
    "        return value\n",
    "\n",
    "# Load a Yandex Click Log File:\n",
    "#   Input:\n",
    "#     file: Yandex Click Log File\n",
    "#   Output: session_map[session_id][query_id] containing alls QuerySession in the Yandex file\n",
    "def load_yandex_click_log(file):\n",
    "    session_map = NestedDict()\n",
    "    qs = None\n",
    "    for line in file:\n",
    "        cells = [cell.strip() for cell in line.split('\\t')]\n",
    "        session_id = int(cells[0])\n",
    "            \n",
    "        if cells[2] == 'Q':\n",
    "            query_id = int(cells[3])\n",
    "            if query_id not in session_map[session_id]:\n",
    "                list_of_urls = [int(cell) for cell in cells[5:]]\n",
    "                doc_list = [Doc(url_id,'None') for url_id in list_of_urls]\n",
    "                qs = QuerySession(doc_list)\n",
    "                qs.click_list = [0] * len(qs.doc_list)\n",
    "                session_map[session_id][query_id] = qs\n",
    "        \n",
    "        if cells[2] == 'C':\n",
    "            url_id = int(cells[3])\n",
    "            for i,doc in enumerate(qs.doc_list):\n",
    "                if doc.id == url_id:\n",
    "                    qs.click_list[i] += 1\n",
    "                    break;\n",
    "    return session_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load file into map\n",
    "f = open('YandexRelPredChallenge.txt', 'r')\n",
    "session_map = load_yandex_click_log(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of QuerySession with session_id: 0 and query_id: 1974\n",
      "Documents: [Doc(1210,HR), Doc(1211,HR), Doc(1212,HR), Doc(1213,HR), Doc(1214,HR)]\n",
      "Clicks per Doc: [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check a QuerySession\n",
    "qs = session_map[0][1974];\n",
    "print('Details of QuerySession with session_id: 0 and query_id: 1974')\n",
    "print('Documents:', doc_list)\n",
    "print('Clicks per Doc:', qs.click_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random click model estimate the probability of a click\n",
    "#   Input:\n",
    "#     session_map[session_id][query_id]\n",
    "#   Output: Rho, the probability of a click\n",
    "def rcm_get_click_prob(session_map):\n",
    "    click_count = 0\n",
    "    doc_count = 0\n",
    "    for query_map in session_map.values():\n",
    "        for session_query in query_map.values():\n",
    "            for clicks in session_query.click_list:\n",
    "                click_count += clicks                  # Count the number of clicks\n",
    "            doc_count += len(session_query.doc_list)   # Count the number of documents\n",
    "    return click_count / doc_count                     # Rho = click_count / doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random click model, estimate whether a document is clicked\n",
    "#   Input:\n",
    "#     rcm_click_prob: the probability of a click\n",
    "#   Output: True if this document was clicked, False otherwise\n",
    "def bernoulli(click_prob):\n",
    "    return random.random() < click_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplified Dependent Click Model estimate the continuation probability\n",
    "#   Input:\n",
    "#     session_map[session_id][query_id]\n",
    "#   Output: Lambda r, a list[rank] with the continuation probability at each rank\n",
    "def sdcm_get_continuation_probs(session_map):\n",
    "    click_list = [0] * 10\n",
    "    click_not_last_list = [0] * 10\n",
    "    for query_map in session_map.values():\n",
    "        for session_query in query_map.values():\n",
    "            click_ranks = [r for r, click in enumerate(session_query.click_list) if click]\n",
    "            if len(click_ranks) > 0:\n",
    "                click_list[click_ranks[-1]] += session_query.click_list[click_ranks[-1]]\n",
    "                for rank in click_ranks[:-1]:\n",
    "                    click_list[rank] += session_query.click_list[rank]\n",
    "                    click_not_last_list[rank] += session_query.click_list[rank]\n",
    "    return [a / b for a, b in zip(click_not_last_list, click_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplified Dependent Click model estimate the attraction probability\n",
    "#   Input:\n",
    "#     query_session\n",
    "#   Output: Alpha r, a list[rank] with the attraction probability at each rank\n",
    "def sdcm_get_attraction_probs(query_session):\n",
    "    return [d.relevance() / 4 for d in query_session.doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplified Dependent Click model, estimate whether a document is clicked\n",
    "#   Input:\n",
    "#     query_session: a query session containing a ranked list of documents\n",
    "#     cont_probs: Lambda r, a list[rank] with the continuation probability at each rank\n",
    "#     attr_probs: Alpha r, a list[rank] with the attraction probability at each rank\n",
    "#   Output: A click list[rank]: [True, False, False]\n",
    "def sdcm_was_clicked(query_session, cont_probs, attr_probs):\n",
    "    exam = 1\n",
    "    click_list = []\n",
    "    for rank in range(len(query_session.doc_list)):\n",
    "        cont = cont_probs[rank]\n",
    "        attr = attr_probs[rank]\n",
    "        if exam == 0:\n",
    "            click = False\n",
    "            exam = 0\n",
    "        else:\n",
    "            click = bernoulli(attr)\n",
    "            exam = bernoulli(1 - click + cont * click)\n",
    "        click_list.append(click)\n",
    "    return click_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate probabilities\n",
    "rcm_click_prob = rcm_get_click_prob(session_map)\n",
    "sdcm_cont_probs = sdcm_get_continuation_probs(session_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random click probability: 0.14\n",
      "Continuation probabilities: [0.37, 0.56, 0.59, 0.58, 0.57, 0.57, 0.54, 0.47, 0.4, 0.0]\n",
      "\n",
      "\n",
      "QuerySession for testing:\n",
      "QuerySession([Doc(0,R), Doc(1,R), Doc(2,N), Doc(3,R), Doc(4,N)])\n",
      "\n",
      "\n",
      "Random Click Model:\n",
      "Clicked-list of test QuerySession: [True, False, False, False, False]\n",
      "\n",
      "\n",
      "Simplified Dependent Click model:\n",
      "Attraction probabilities of test-QuerySession: [0.25, 0.25, 0.0, 0.25, 0.0]\n",
      "Clicked-list of test-QuerySession: [False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Check probabilities\n",
    "print('Random click probability:', round(rcm_click_prob, 2))\n",
    "print('Continuation probabilities:', [round(cp, 2) for cp in sdcm_cont_probs])\n",
    "print('\\n')\n",
    "\n",
    "# test-QuerySession\n",
    "qs = QuerySession([Doc(0,'R'),Doc(1,'R'),Doc(2,'N'),Doc(3,'R'),Doc(4,'N')])\n",
    "print('QuerySession for testing:')\n",
    "print(qs)\n",
    "print('\\n')\n",
    "\n",
    "# Test Random Click Model\n",
    "rdm_click_list = [bernoulli(rcm_click_prob) for i in range(len(qs.doc_list))]\n",
    "print('Random Click Model:')\n",
    "print('Clicked-list of test QuerySession:', rdm_click_list)\n",
    "print('\\n')\n",
    "\n",
    "# Test Simplified Dependent Click Model\n",
    "sdcm_attr_probs = sdcm_get_attraction_probs(qs)\n",
    "sdcm_click_list = sdcm_was_clicked(qs, sdcm_cont_probs, sdcm_attr_probs)\n",
    "print('Simplified Dependent Click model:')\n",
    "print('Attraction probabilities of test-QuerySession:', sdcm_attr_probs)\n",
    "print('Clicked-list of test-QuerySession:', sdcm_click_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Simulate Interleaving Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run Random Click Model on interleaved list\n",
    "#   Input:\n",
    "#     interleaf: {'interleaved':i_list, 'team_a':team_a, 'team_b':team_b}\n",
    "#     n: number of runs\n",
    "#   Output: proportion p of clicks for E\n",
    "def run_rcm(interleaf, n=1000):\n",
    "    interleaved = interleaf['interleaved']\n",
    "    team_a = interleaf['team_a']\n",
    "    team_b = interleaf['team_b']\n",
    "    \n",
    "    p_click_count = 0\n",
    "    e_click_count = 0\n",
    "    for i in range(n):\n",
    "        for doc in interleaved.doc_list:\n",
    "            if bernoulli(rcm_click_prob):\n",
    "                if doc in team_a:\n",
    "                    p_click_count += 1\n",
    "                if doc in team_b:\n",
    "                    e_click_count += 1\n",
    "\n",
    "    total_count = p_click_count + e_click_count\n",
    "    return 0 if total_count == 0 else e_click_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Simplified Dependend Click Model on interleaved list\n",
    "#   Input:\n",
    "#     interleaf: {'interleaved':i_list, 'team_a':team_a, 'team_b':team_b}\n",
    "#     n: number of runs\n",
    "#   Output: proportion p of clicks for E\n",
    "def run_sdcm(interleaf, n=1000):\n",
    "    interleaved = interleaf['interleaved']\n",
    "    team_a = interleaf['team_a']\n",
    "    team_b = interleaf['team_b']\n",
    "    \n",
    "    p_click_count = 0\n",
    "    e_click_count = 0\n",
    "    for i in range(n):\n",
    "        sdcm_attr_probs = sdcm_get_attraction_probs(interleaved)\n",
    "        click_list = sdcm_was_clicked(interleaved, sdcm_cont_probs, sdcm_attr_probs)\n",
    "        for rank, click in enumerate(click_list):\n",
    "            if click:\n",
    "                doc = interleaved.doc_list[rank]\n",
    "                if doc in team_a:\n",
    "                    p_click_count += 1\n",
    "                if doc in team_b:\n",
    "                    e_click_count += 1\n",
    "\n",
    "    total_count = p_click_count + e_click_count\n",
    "    return 0 if total_count == 0 else e_click_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "for pair in pairs:\n",
    "    pair.click_prop_td_rcm = run_rcm(pair.td_interleaving)\n",
    "    pair.click_prop_td_sdcm = run_sdcm(pair.td_interleaving)\n",
    "    #pair.click_prop_p_rcm = run_rcm(pair.p_interleaving)\n",
    "    #pair.click_prop_p_sdcm = run_sdcm(pair.p_interleaving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Pearson correlation coefficient between measures and click-proportions\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "ap_measures = []\n",
    "dcg_measures = []\n",
    "rbp_measures = []\n",
    "err_measures = []\n",
    "click_prop_td_rcms = []\n",
    "click_prop_td_sdcms = []\n",
    "\n",
    "for pair in pairs:\n",
    "    ap_measures += [pair.ap_measure]\n",
    "    dcg_measures += [pair.dcg_measure]\n",
    "    rbp_measures += [pair.rbp_measure]\n",
    "    err_measures += [pair.err_measure]\n",
    "    click_prop_td_rcms += [pair.click_prop_td_rcm]\n",
    "    click_prop_td_sdcms += [pair.click_prop_td_sdcm]\n",
    "\n",
    "pcc_ap_rcm = pearsonr(ap_measures, click_prop_td_rcms)\n",
    "pcc_dcg_rcm = pearsonr(dcg_measures, click_prop_td_rcms)\n",
    "pcc_rbp_rcm = pearsonr(rbp_measures, click_prop_td_rcms)\n",
    "pcc_err_rcm = pearsonr(err_measures, click_prop_td_rcms)\n",
    "pcc_ap_sdcm = pearsonr(ap_measures, click_prop_td_sdcms)\n",
    "pcc_dcg_sdcm = pearsonr(dcg_measures, click_prop_td_sdcms)\n",
    "pcc_rbp_sdcm = pearsonr(rbp_measures, click_prop_td_sdcms)\n",
    "pcc_err_sdcm = pearsonr(err_measures, click_prop_td_sdcms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient between measures and click-proportions using Team-Draft Interleaving:\n",
      "\n",
      "Average Precision (AP)           and Random Click Model (RCM):                -0.00127711369941\n",
      "Expected Reciprocal Rank (ERR)   and Random Click Model (RCM):                -0.00035283570723\n",
      "Discounted Cumulative Gain (DCG) and Random Click Model (RCM):                0.00125283413329\n",
      "Rank Biased Precision (RBP)      and Random Click Model (RCM):                0.000947231375213\n",
      "Average Precision (AP)           and Simplified Dependent Click model (SDCM): 0.616618771709\n",
      "Expected Reciprocal Rank (ERR)   and Simplified Dependent Click model (SDCM): 0.834751718387\n",
      "Discounted Cumulative Gain (DCG) and Simplified Dependent Click model (SDCM): 0.848015207088\n",
      "Rank Biased Precision (RBP)      and Simplified Dependent Click model (SDCM): 0.856530398587\n"
     ]
    }
   ],
   "source": [
    "print('Pearson correlation coefficient between measures and click-proportions using Team-Draft Interleaving:\\n')\n",
    "print('Average Precision (AP)           and Random Click Model (RCM):               ', pcc_ap_rcm[0])\n",
    "print('Expected Reciprocal Rank (ERR)   and Random Click Model (RCM):               ', pcc_err_rcm[0])\n",
    "print('Discounted Cumulative Gain (DCG) and Random Click Model (RCM):               ', pcc_dcg_rcm[0])\n",
    "print('Rank Biased Precision (RBP)      and Random Click Model (RCM):               ', pcc_rbp_rcm[0])\n",
    "print('Average Precision (AP)           and Simplified Dependent Click model (SDCM):', pcc_ap_sdcm[0])\n",
    "print('Expected Reciprocal Rank (ERR)   and Simplified Dependent Click model (SDCM):', pcc_err_sdcm[0])\n",
    "print('Discounted Cumulative Gain (DCG) and Simplified Dependent Click model (SDCM):', pcc_dcg_sdcm[0])\n",
    "print('Rank Biased Precision (RBP)      and Simplified Dependent Click model (SDCM):', pcc_rbp_sdcm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "In this research we simulated two non-existing information retrieval algorithms. The algorithm in production, called P, and an experimental algorithm, called E. Team-draft Interleaving was used to setup an experiment between E and P. In this experiment, we tested four evaluation methods and two click models.\n",
    "\n",
    "The tested click models:\n",
    "- Random Click Model (RCM)\n",
    "- Simplified Dependent Click Model (SDCM)\n",
    "\n",
    "The tested evaluation metods:\n",
    "- Average Precision (AP)\n",
    "- Discounted Cumulative Gain (DCG)\n",
    "- Rank Biased Precision (RBP)\n",
    "- Expected Reciprocal Rank (ERR)\n",
    "\n",
    "Algorithm E and P where evaluated using all evaluation methods. By taking the difference of the evaluations, we calculated the measure. The measure is expected to be positive when E has more relavant document on top of the ranking list then P.\n",
    "Then, every click model was tested on the interleaved ranking. This resulted in a proportion of clicks for E. The proportion of clicks for E is expected to be 'high' when E has more relavant document on top of the ranking list then P. \n",
    "\n",
    "Both the measure and the proportion of clicks are expected to be 'high' when E has more relevant documents onn top of the ranking list, then P. This means that the measure and the proportion of click are correlated. When a 'good' click model is evaluated with a 'good' evaluation method, the correlation will approach 1. When a either the click model is 'bad' or when the evaluation method is 'bad', the correlation will approach 0. \n",
    "\n",
    "The RCM does a bad job at simulating user clicks. This can be seen in the results. The correlation coefficient of RCM approaches 0 at all of the evaluation methods.\n",
    "\n",
    "\n",
    "The SDCM does a better job at simulating user clicks. The correlation coefficient of SDCM is much closer to 1.\n",
    "\n",
    "If we assume that the SDCM is 'good' at simulating user clicks, we can also evaluate the evaluation methods. AP is bad at evaluating the SDCM. Since the correlation between SDCM and AP is lower then the correlation between SDCM and the other evaluation methods.\n",
    "\n",
    "Finally, SDCM and RBP have the highest correlation. This sugests SDCM is the best click model and that RBP is the best evaluation method. However, the significance of this result, has not been measured. And since the correlation between SDCM and ERR,DCG,RBP, are all more or less similar, the result should be taken with a grain of salt."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
